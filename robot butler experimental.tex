
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Section 4.2, `Simple Mario'

\subsection{Robot Butler}

In the RB domain, the robot has to travel to the location of a person who 
has no beverage and serve them a drink. As described in Section~3, 
\textit{move} actions move the robot between rooms, while \textit{serve} 
actions result in a person at the robot's location to have a beverage. 
Continuing for one hour (six actions), failing to serve a drink, or serving 
both people, terminates the episode.

As well as the domain objects being in different locations, scenarios in the RB 
domain can vary by a number of attributes. 
Each person has one of three different \textit{roles} and each location is one 
of four different types of \textit{room}. The scenario can take place either 
\textit{early} or \textit{late} in the week. 
The objective of the robot butler is to serve drinks to the people in a way 
that will not result in episode termination -- any such unexpected termination 
triggers RRL for discovering axioms.

To evaluate the ability to efficiently discover generic domain axioms in the RB 
domain, we use an approach similar to that used in the BW domain. If any plan 
step results in an unexpected failure that cannot be explained using existing 
knowledge, scenarios similar to the one causing the failure (e.g., with different 
roles for people or types of room) are simulated automatically (see Section 3.2). 
These simulated scenarios provide the training examples necessary for 
generalizing from the specific axioms discovered. 

Figure~\ref{?} compares the rate of convergence of Q-RRL and Q-learning as 
a function of the number of episodes. Incremental Q-RRL converges faster than 
Q-learning, although the difference is less large than in the Blocks World example 
shown earlier. 

We tested the system on the RB domain with missing constraints, 
namely that managers only accept beverages if it is early in the week; 
and either that drinks cannot be served in the 
workshop or that they can \textit{only} be served in the kitchen.

The system was less accurate than it was for Blocks World, 
possibly due to having multiple concurrent missing axioms. 
Often the sole highest-valued candidate was not a correct executability constraint, 
but many of the higher-valued candidates did tend to be close to correct, often 
being either too general or carrying additional unnecessary attributes.

We list some examples of axioms that the framework generated.

$\neg occurs(serve(A,B,C)) :- earlyinweek(false), \neg role(B,engineer), \neg role(B,salesperson)$

Note that specifying that B is not an engineer or salesperson is a 
convoluted way of saying they are a manager, so this is correct.

We also get variations such as 

$\neg occurs(serve(A,B,C)) :- earlyinweek(false)$

which are not specific enough.

When learning that people cannot be served in a room that is a workshop, the system produced
axioms such as 

$\neg occurs(serve(A,B,C)) :- \neg roomtype(C,conference\_room), \neg roomtype(C,kitchen)$

which is missing a case for the office. The axiom

$\neg occurs(serve(A,B,C)) :- earlyinweek(true), role(p2, engineer), \neg roomtype(C, conference\_room),
\neg roomtype(C, kitchen), \neg roomtype(C, office\_space)$

did find all the cases but has additional unnecessary clauses.

In the more specific case that people can only be served in a room that is a kitchen, 
axioms included the overly narrow

$\neg occurs(serve(A,B,C)) :- roomtype(C,office\_space), \neg roomtype(C,workshop)$

and

$\neg occurs(serve(A,B,C)) :- roomtype(C, office\_space), \neg role(B, salesperson)$

as well as 

$\neg occurs(serve(A,B,C)) :- earlyinweek(false), \neg roomtype(C, kitchen)$

%

We would expect that when the system is unable to find the perfectly matching 
generalised axiom, the value it would assign to the leading candidates is then lower. 
This is indeed what we observe in practise, and it indicates the system's 
ability to differentiate between candidates in discovering previously unknown axioms. 

%%%%%%%%%%%%%%%%%%%%

% LIST OF ACTUAL EXAMPLES GENERATED FOR BLOCKS WORLD
% TO BE USED EARLIER IN PAPER

%finalexample(action(puton(grip1, b2, b1)), [[], [colour(b1, green)]], 10.0).
%finalexample(action(puton(grip1, b2, b1)), [[], [colour(b1, green), shape(b3, triangleprism)]], 7.440476190476192).
%finalexample(action(puton(grip1, b2, b1)), [[colour(b2, blue)], []], 8.455882352941176).
%finalexample(action(puton(grip1, b2, b1)), [[shape(b3, cube)], [colour(b3, blue)]], 8.5).
%finalexample(action(puton(grip1, b2, b1)), [[shape(b1, triangleprism)], []], 10.000518429239477).
%finalexample(action(puton(grip1, b2, b1)), [[colour(b4, red), shape(b4, triangleprism)], []], 9.166666666666668).
%finalexample(action(puton(grip1, b2, b1)), [[], [colour(b4, green)]], 8.012820512820513).
%
$\neg occurs(puton(A,B,C)) :- shape(C,triangleprism)$



%%%%%%%%%%%%%%%%%%%%

% RESULTS SUPPORTING SUPERIORITY OF Q-RRL

% example counts, two runs each
% RRL, BW	|	439 / 297		|	368 (500-1000 nodes total)
% RRL, RB	|	235 / 224		|	229	(400-500 nodes total)
% Q-L, BW	|	20155 / 14814	|	17484
% Q-L, RB	|	1934 / 1977		|	1955

For basic Q-Learning, the data for an example (including its Q-Value) is stored 
in a record. For Q-RRL, it is stored in a leaf. 
One of the motivations for the relational reinforcement learning work by 
Driessens et al. (2001) is that it is expensive to keep track of a monotonically 
increasing number of examples, and building trees from this anew each episode 
becomes correspondingly more costly.  
Memory costs can be estimated by counting the number of records and leaves 
generated for Q-Learning and Q-RRL respectively, over all MDPs produced in the 
course of learning. 
We find that using Q-RRL with incremental tree learning reduces the number of 
records drastically: by between 89\% (for RB) and 98\% (for BW). 

Even if we count all nodes in the BDTs towards the complexity of the framework, 
using Q-RRL results in a reduction in complexity of at least 75\%.
This is supported by an observed reduction in processing time when we use Q-RRL.

%500/17484=0.029
%1000/17484=0.057
%400/1955=0.205
%500/1955=0.256

%%%%%%%%%%%%%%%%%%%%

